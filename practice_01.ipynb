{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1장 케라스 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "print(keras.backend.backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([0, 1, 2, 3, 4])\n",
    "y = x * 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(1, input_shape=(1, )))\n",
    "model.compile(optimizer='SGD', loss='mse')\n",
    "\n",
    "model.fit(x[:2], y[:2], epochs=1000, verbose=0)\n",
    "\n",
    "print('Targets:', y[2:])\n",
    "print('Predictions:', model.predict(x[2:]).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2장 ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분산 방식 모델링, 함수형 구현\n",
    "from keras import layers, models\n",
    "\n",
    "x = layers.Input(shape=(None, ))\n",
    "h = layers.Activation('relu')(layers.Dense(None)(x))\n",
    "y = layers.Activation('softmax')(layers.Dense(None)(h))\n",
    "\n",
    "model = models.Model(x, y)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연쇄 방식 모델링, 함수형 구현\n",
    "model = models.Sequential() # 모델 구조 정의 전 Sequential로 초기화\n",
    "model.add(layers.Dense(None, activation='relu', input_shape=(None, )))\n",
    "model.add(layers.Dense(None, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분산 방식 모델링, 객체지향형 구현\n",
    "class ANN(models.Model):\n",
    "    def __init__(self, input_num, hidden_num, output_num, **kwargs):\n",
    "        hidden = layers.Dense(hidden_num)\n",
    "        output = layers.Dense(output_num)\n",
    "        relu = layers.Activation('relu')\n",
    "        softmax = layers.Activation('softmax')\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "model = ANN(input_num, hidden_num, output_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연쇄 방식 모델링, 객체지향형 구현\n",
    "class ANN(models.Sequential):\n",
    "    def __init__(self, input_num, hidden_num, output_num, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.add(layers.Dense(hidden_num, activation='relu', input_shape=(input_num, )))\n",
    "        self.add(layers.Dense(output_num, activation='softmax'))\n",
    "        self.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from keras import datasets  #mnist\n",
    "from keras.datasets import mnist    # 이렇게 불러와야 실행됨\n",
    "from keras.utils import np_utils # to_categorical\n",
    "\n",
    "def Data_func():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "    L, W, H = X_train.shape\n",
    "    X_train = X_train.reshape(-1, W*H)\n",
    "    X_test = X_test.reshape(-1, W*H)\n",
    "\n",
    "    X_train = X_train/255.0\n",
    "    X_test = X_test/255.0\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc=0)\n",
    "\n",
    "def plot_acc(history):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_num = 784\n",
    "hidden_num = 100\n",
    "number_of_class = 10\n",
    "output_num = number_of_class\n",
    "model = ANN(input_num, hidden_num, output_num)\n",
    "(X_train, y_train), (X_test, y_test) = Data_func()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=100, validation_split=0.2)\n",
    "\n",
    "performance_test = model.evaluate(X_test, y_test, batch_size=100)\n",
    "print('Test loss and Accuracy -> {: .2f}, {: .2f}'.format(*performance_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)\n",
    "plt.show()\n",
    "plot_acc(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시계열\n",
    "from keras import layers, models\n",
    "\n",
    "class ANN(models.Model):\n",
    "    def __init__(self, Nin, Nh, Nout):\n",
    "        hidden = layers.Dense(Nh)\n",
    "        output = layers.Dense(Nout)\n",
    "        relu = layers.Activation('relu')\n",
    "\n",
    "        x = layers.Input(shape=(Nin, ))\n",
    "        h = relu(hidden(x))\n",
    "        y = output(h)\n",
    "\n",
    "        super.__init__(x, y)\n",
    "        self.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "# 보스턴 집값 데이터\n",
    "from keras.datasets import boston_housing\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def Data_func():\n",
    "    (X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "# 시각화\n",
    "from original.keraspp.skeras import plot_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main():\n",
    "    Nin = 13\n",
    "    Nh = 5\n",
    "    Nout = 1\n",
    "\n",
    "    model = ANN(Nin, Nh, Nout)\n",
    "    (X_train, y_train), (X_test, y_test) = Data_func()\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=100, validation_split=0.2, verbose=2)\n",
    "\n",
    "    performance_test = model.evaluate(X_test, y_test, batch_size=100)\n",
    "    print('\\nTest Loss -> {: .2f}'.format(performance_test))\n",
    "\n",
    "    plot_loss()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3장 DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "\n",
    "Nin = 784\n",
    "Nh_l = [100, 50]\n",
    "number_of_class = 10\n",
    "Nout = number_of_class\n",
    "\n",
    "class DNN(models.Sequential):\n",
    "    def __init__(self, Nin, Nh_1, Nout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.add(layers.Dense(Nh_1[0], activation='relu', input_shape=(Nin, ), name='Hidden-1'))\n",
    "        self.add(layers.Dropout(0.2))\n",
    "        self.add(layers.Dense(Nh_1[1], activation='relu', name='Hidden-2'))\n",
    "        self.add(layers.Dropout(0.2))\n",
    "        self.add(layers.Dense(Nout, activation='softmax'))\n",
    "\n",
    "        self.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils # to_categorical\n",
    "\n",
    "def Data_func():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "    L, W, H = X_train.shape\n",
    "    X_train = X_train.reshape(-1, W*H)\n",
    "    X_test = X_test.reshape(-1, W*H)\n",
    "\n",
    "    X_train = X_train/255.0\n",
    "    X_test = X_test/255.0\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = Data_func()\n",
    "\n",
    "model = DNN(Nin, Nh_l, Nout)\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=100, validation_split=0.2)\n",
    "performance_test = model.evaluate(X_test, y_test, batch_size=100)\n",
    "print('Test Loss and Accuracy ->', performance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬러 이미지\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def Data_func():\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "    L, W, H, C = X_train.shape\n",
    "    X_train = X_train.reshape(-1, W*H*C)\n",
    "    X_test = X_test.reshape(-1, W*H*C)\n",
    "\n",
    "    X_train = X_train/255.0\n",
    "    X_test = X_test/255.0\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "\n",
    "# 모델링\n",
    "from keras import layers, models\n",
    "\n",
    "class DNN(models.Sequential):\n",
    "    def __init__(self, Nin, Nh_l, Pd_l, Nout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.add(layers.Dense(Nh_l[0], activation='relu', input_shape=(Nin, ), name='Hiiden-1'))\n",
    "        self.add(layers.Dropout(Pd_l[0]))\n",
    "        self.add(layers.Dense(Nh_l[1], activation='relu', input_shape=(Nin, ), name='Hiiden-2'))\n",
    "        self.add(layers.Dropout(Pd_l[1]))\n",
    "        self.add(layers.Dense(Nout, activation='softmax'))\n",
    "        self.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# 학습 및 시각화\n",
    "from original.keraspp.skeras import plot_loss, plot_acc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Nh_l = [100, 50]\n",
    "Pd_l = [0.0, 0.0]\n",
    "number_of_class = 10\n",
    "Nout = number_of_class\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = Data_func()\n",
    "model = DNN(X_train.shape[1], Nh_l, Pd_l, Nout)\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=100, validation_split=0.2)\n",
    "performance_test = model.evaluate(X_test, y_test, batch_size=100)\n",
    "print('Test Loss and Accuracy ->', performance_test)\n",
    "\n",
    "plot_acc(history)\n",
    "plt.show(block=False)\n",
    "plt.pause(2)\n",
    "plt.close()\n",
    "plot_loss(history)\n",
    "plt.show(block=False)\n",
    "plt.pause(2)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4장 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models, layers\n",
    "import keras.backend\n",
    "import keras.optimizers\n",
    "\n",
    "class CNN(models.Sequential):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolutionL 특징점 찾기\n",
    "        self.add(layers.Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=input_shape))\n",
    "        self.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "        self.add(layers.MaxPooling2D(pool_size=(2,2)))  # pool_size: 1개의 값으로 변화할 크기\n",
    "        self.add(layers.Dropout(0.25))\n",
    "        self.add(layers.Flatten())\n",
    "\n",
    "        # dense: 실질적 분류 작업\n",
    "        self.add(layers.Dense(128, activation='relu'))\n",
    "        self.add(layers.Dropout(0.5))\n",
    "        self.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        self.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['acc'])\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 채널의 위치는 image_data_format에 따름\n",
    "print(keras.backend.backend_config.image_data_format())\n",
    "\n",
    "# 1 = 채널의 크기\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "# X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "X_test = X_test.reshape(*X_test.shape, 1)\n",
    "input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
    "\n",
    "## 채널의 위치가 앞 단에 존재할 경우\n",
    "# X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
    "# X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2])\n",
    "# input_shape = (1, X_train.shape[1], X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models, layers\n",
    "import keras.backend\n",
    "import keras.optimizers\n",
    "\n",
    "class CNN(models.Sequential):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolutionL 특징점 찾기\n",
    "        self.add(layers.Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=input_shape))\n",
    "        self.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "        self.add(layers.MaxPooling2D(pool_size=(2,2)))  # pool_size: 1개의 값으로 변화할 크기\n",
    "        self.add(layers.Dropout(0.25))\n",
    "        self.add(layers.Flatten())\n",
    "\n",
    "        # dense: 실질적 분류 작업\n",
    "        self.add(layers.Dense(128, activation='relu'))\n",
    "        self.add(layers.Dropout(0.5))\n",
    "        self.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        self.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['acc'])\n",
    "\n",
    "from keras.datasets import mnist\n",
    "class DATA():\n",
    "    def __init__(self):\n",
    "        num_classes = 10\n",
    "\n",
    "        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "        \n",
    "        # 채널의 위치는 image_data_format에 따름\n",
    "        print(keras.backend.backend_config.image_data_format())\n",
    "\n",
    "        # 1 = 채널의 크기\n",
    "        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "        # X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "        X_test = X_test.reshape(*X_test.shape, 1)\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
    "\n",
    "        ## 채널의 위치가 앞 단에 존재할 경우\n",
    "        # X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
    "        # X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2])\n",
    "        # input_shape = (1, X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "        X_train = X_train.astype('float32')\n",
    "        X_test = X_test.astype('float32')\n",
    "        X_train /= 255\n",
    "        X_test /= 255\n",
    "\n",
    "        y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "        y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_test, self.y_test = X_test, y_test\n",
    "\n",
    "data = DATA()\n",
    "model = CNN(data.input_shape, data.num_classes)\n",
    "history = model.fit(data.X_train, data.y_train, batch_size=128, epochs=10, validation_split=0.2)\n",
    "score = model.evaluate(data.X_test, data.y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "plot_acc(history)\n",
    "plt.show(block=False)\n",
    "plt.pause(2)\n",
    "plt.close()\n",
    "\n",
    "plot_loss(history)\n",
    "plt.show(block=False)\n",
    "plt.pause(2)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬러\n",
    "import keras.backend\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from original.keraspp import skeras, sfile\n",
    "\n",
    "class CNN(Model):\n",
    "    def __init__(model, nb_classes, in_shape=None):\n",
    "        model.nb_classes = nb_classes\n",
    "        model.in_shape = in_shape\n",
    "        model.build_model()\n",
    "        super().__init__(model.x, model.y)\n",
    "        model.cl_part = Model(model.x, model.z_cl)\n",
    "        model.fl_part = Model(model.x, model.z_fl)\n",
    "        model.compile()\n",
    "    \n",
    "    def build_model(model):\n",
    "        nb_classes = model.nb_classes\n",
    "        in_shape = model.in_shape\n",
    "\n",
    "        x = Input(in_shape)\n",
    "        h = Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=in_shape)(x)\n",
    "        h = Conv2D(64, (3,3), activation='relu')(h)\n",
    "        h = MaxPooling2D(pool_size=(2,2))(h)\n",
    "        h = Dropout(0.25)(h)\n",
    "        h = Flatten()(h)\n",
    "        z_cl = h\n",
    "\n",
    "        h = Dense(128, activation='relu')(h)\n",
    "        h = Dropout(0.5)(h)\n",
    "        z_fl = h\n",
    "\n",
    "        y = Dense(nb_classes, activation='softmax', name='preds')(h)\n",
    "        model.z_cl = z_cl\n",
    "        model.z_fl = z_fl\n",
    "        model.x, model.y = x, y\n",
    "\n",
    "    def compile(model):\n",
    "        Model.compile(model, loss='categorical_crossentropy', optimizer='adadelta', metrics=['acc'])\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self, X, y, nb_classes, scaling=True, test_size=0.2, random_state=0):\n",
    "        self.X = X\n",
    "        self.add_channels()\n",
    "        X = self.X\n",
    "\n",
    "        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "        X_train = X_train.astype('float32')\n",
    "        X_test = X_test.astype('float32')\n",
    "\n",
    "        if scaling:\n",
    "            # scaling to have (0, 1) for each feature (each pixel)\n",
    "            scaler = MinMaxScaler()\n",
    "            n = X_train.shape[0]\n",
    "            X_train = scaler.fit_transform(X_train.reshape(n, -1)).reshape(X_train.shape)\n",
    "            n = X_test.shape[0]\n",
    "            X_test = scaler.transform(X_test.reshape(n, -1)).reshape(X_test.shape)\n",
    "            self.scaler = scaler\n",
    "        \n",
    "        y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "        y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "        \n",
    "        self.X_train, self.X_test = X_train, X_test\n",
    "        self.y_train, self.y_test = y_train, y_test\n",
    "    \n",
    "    def add_channels(self):\n",
    "        X = self.X\n",
    "\n",
    "        if len(X.shape) == 3:\n",
    "            N, img_rows, img_cols = X.shape\n",
    "\n",
    "            if K.image_data_format() == 'channels_first':\n",
    "                X = X.reshape(X.shape[0], 1, img_rows, img_cols)\n",
    "                input_shape = (1, img_rows, img_cols)\n",
    "            else:\n",
    "                X = X.reshape(X.shape[0], img_rows, img_cols, 1)\n",
    "                input_shape = (img_rows, img_cols, 1)\n",
    "        else:\n",
    "            input_shape = X.shape[1:]   # channel is already included\n",
    "        \n",
    "        self.X = X\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "class Machine():\n",
    "    def __init__(self, X, y, nb_classes=2, fig=True):\n",
    "        self.nb_classes = nb_classes\n",
    "        self.set_data(X, y)\n",
    "        self.set_model()\n",
    "        self.fig = fig\n",
    "    \n",
    "    def set_data(self, X, y):\n",
    "        nb_classes = self.nb_classes\n",
    "        self.data = DataSet(X, y, nb_classes)\n",
    "    \n",
    "    def set_model(self):\n",
    "        nb_classes = self.nb_classes\n",
    "        data = self.data\n",
    "        self.model = CNN(nb_classes=nb_classes, in_shape=data.input_shape)\n",
    "    \n",
    "    def fit(self, nb_epoch=10, batch_size=128, verbose=1):\n",
    "        data = self.data\n",
    "        model = self.model\n",
    "        history = model.fit(data.X_train, data.y_train, batch_size=batch_size, epochs=nb_epoch, verbose=verbose, validation_data=(data.X_test, data.y_test))\n",
    "        return history\n",
    "\n",
    "    def run(self, nb_epoch=10, batch_size=128, verbose=1):\n",
    "        data = self.data\n",
    "        model = self.model\n",
    "        fig = self.fig\n",
    "    \n",
    "        history = self.fit(nb_epoch=nb_epoch, batch_size=batch_size, verbose=verbose)\n",
    "        score = model.evaluate(data.X_test, data.y_test, verbose=0)\n",
    "\n",
    "        print('Confusion matrix')\n",
    "        y_test_pred = model.predict(data.X_test, verbose=0)\n",
    "        y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "        print(metrics.confusion_matrix(np.argmax(data.y_test, axis=1), y_test_pred))\n",
    "\n",
    "        print('Test score:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        suffix = sfile.unique_filename('datatime')\n",
    "        foldname = 'output_' + suffix\n",
    "        os.makedirs(foldname)\n",
    "        skeras.save_history_history('history_history.npy', history.history, fold=foldname)\n",
    "        model.save_weights(os.path.join(foldname, 'dl_model.h5'))\n",
    "        print('Output results are saved in', foldname)\n",
    "\n",
    "        if fig:\n",
    "            plt.figure(figsize=(12,4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            skeras.plot_acc(history)\n",
    "            plt.subplot(1, 2, 2)\n",
    "            skeras.plot_loss(history)\n",
    "            plot_acc(history)\n",
    "            plt.show(block=False)\n",
    "            plt.pause(2)\n",
    "            plt.close()\n",
    "\n",
    "        self.history = history\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "import keras\n",
    "assert keras.backend.image_data_format() == 'channels_last'\n",
    "from original.keraspp import aicnn\n",
    "\n",
    "class Machine(Machine):\n",
    "    def __init__(self):\n",
    "        (X, y), (X_test, y_test) = cifar10.load_data()\n",
    "        super().__init__(X=X, y=y, nb_classes=10)\n",
    "    \n",
    "def main():\n",
    "    m = Machine()\n",
    "    m.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5장 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function   # 파이썬 2와 3 간의 호환성 위한 패키지\n",
    "# 파이썬 2에선 print를 괄호 없이 사용하지만 이 함수를 호출함으로써 함수로써(괄호와 함꼐) 사용하게 함\n",
    "\n",
    "# 자연어\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import pad_sequences\n",
    "from keras.datasets import imdb\n",
    "from keras import layers, models\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, max_features=20000, maxlen=80):\n",
    "        (x_train, self.y_train), (x_test, self.y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "        # x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "        # x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "        # pad_sequences의 경로가 바뀜\n",
    "        self.x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "        self.x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "class RNN_LSTM(models.Model):\n",
    "    def __init__(self, max_features, maxlen):\n",
    "        # model = models.Sequential()\n",
    "        # model.add(layers.Embedding(max_features, 128))\n",
    "        # model.add(layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "        # model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "        x = layers.Input((maxlen, ))\n",
    "        h = layers.Embedding(max_features, 128)(x)\n",
    "        h = layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2)(h)\n",
    "        y = layers.Dense(1, activation='sigmoid')(h)\n",
    "        super().__init__(x, y)\n",
    "        self.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "class Machine:\n",
    "    def __init__(self, max_features=20000, maxlen=80):\n",
    "        self.data = Data(max_features, maxlen)\n",
    "        self.model = RNN_LSTM(max_features, maxlen)\n",
    "    \n",
    "    def run(self, epochs=3, batch_size=32):\n",
    "        data = self.data\n",
    "        model = self.model\n",
    "\n",
    "        print('Training stage')\n",
    "        print('====================')\n",
    "        model.fit(data.x_train, data.y_train, batch_size=batch_size, epochs=epochs, validation_data=(data.x_test, data.y_test))\n",
    "        score, acc = model.evaluate(data.x_test, data.y_test, batch_size=batch_size)\n",
    "        print('Test performance: accuracy{0}, loss={1}'.format(acc, score))\n",
    "\n",
    "def main():\n",
    "    m = Machine()\n",
    "    m.run(batch_size=512)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시계열\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from keras import models, layers\n",
    "\n",
    "from original.keraspp import skeras\n",
    "\n",
    "def main():\n",
    "    machine = Machine()\n",
    "    machine.run(epochs=400)\n",
    "\n",
    "class Machine():\n",
    "    def __init__(self):\n",
    "        self.data = Dataset()\n",
    "        shape = self.data.X.shape[1:]\n",
    "        self.model = rnn_model(shape)\n",
    "    \n",
    "    def run(self, epochs=400):\n",
    "        d = self.data\n",
    "        X_train, X_test = d.X_train, d.X_test\n",
    "        y_train, y_test = d.y_train, d.y_test\n",
    "        X, y = d.X, d.y\n",
    "        \n",
    "        m = self.model\n",
    "        h = m.fit(X_train, y_train, epochs=epochs, validation_data=[X_test, y_test], verbose=0)\n",
    "        skeras.plot_loss(h)\n",
    "        plt.title('History of training')\n",
    "        plt.show(block=False)\n",
    "        plt.pause(2)\n",
    "        plt.close()\n",
    "\n",
    "        yp = m.predict(X_test)\n",
    "        print('Loss:', m.evaluate(X_test, y_test))\n",
    "        plt.plot(yp, label='Original')\n",
    "        plt.plot(y_test, label='Prediction')\n",
    "        plt.legend(loc=0)\n",
    "        plt.title('Validation Results')\n",
    "        plt.show(block=False)\n",
    "        plt.pause(2)\n",
    "        plt.close()\n",
    "\n",
    "        yp = m.predict(X_test).reshape(-1)\n",
    "        print('Loss:', m.evaluate(X_test, y_test))\n",
    "        print(yp.shape, y_test.shape)\n",
    "        df = pd.DataFrame()\n",
    "        df['Sample'] = list(range(len(y_test))) * 2\n",
    "        df['Normalized #Passengers'] = np.concatenate([y_test, yp], axis=0)\n",
    "        df['Type'] = ['Original'] * len(y_test) + ['Prediction'] * len(yp)\n",
    "\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        sns.barplot(x='Sample', y='Normalized #Passengers', hue='Type', data=df)\n",
    "        plt.ylabel('Normalized #Passengers')\n",
    "        plt.show(block=False)\n",
    "        plt.pause(2)\n",
    "        plt.close()\n",
    "\n",
    "        yp = m.predict(X)\n",
    "        plt.plot(yp, label='Original')\n",
    "        plt.plot(y, label='Prediction')\n",
    "        plt.legend(loc=0)\n",
    "        plt.title('All Results')\n",
    "        plt.show(block=False)\n",
    "        plt.pause(2)\n",
    "        plt.close()\n",
    "\n",
    "def rnn_model(shape):\n",
    "    m_x = layers.Input(shape=shape)\n",
    "    m_h = layers.LSTM(10)(m_x)\n",
    "    m_y = layers.Dense(1)(m_h)\n",
    "    m = models.Model(m_x, m_y)\n",
    "    m.compile('adam', 'mean_squared_error')\n",
    "    m.summary()\n",
    "\n",
    "    return m\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, fname='original/international-airline-passengers.csv', D=12):\n",
    "        data_dn = load_data(fname=fname)\n",
    "        X, y = get_Xy(data_dn, D=D)\n",
    "        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.X, self.y = X, y\n",
    "        self.X_train, self.X_test = X_train, X_test\n",
    "        self.y_train, self.y_test = y_train, y_test\n",
    "    \n",
    "def load_data(fname='original/international-airline-passengers.csv'):\n",
    "    dataset = pd.read_csv(fname, usecols=[1], engine='python', skipfooter=3)\n",
    "    data = dataset.values.reshape(-1)\n",
    "\n",
    "    plt.plot(data)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('#Passengers')\n",
    "    plt.title('Original Data')\n",
    "    plt.show(block=False)\n",
    "    plt.pause(2)\n",
    "    plt.close()\n",
    "\n",
    "    data_dn = (data - np.mean(data)) / np.std(data) / 5\n",
    "    plt.plot(data_dn)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Normalized #Passengers')\n",
    "    plt.title('Normalized data by $E[]$ and $5\\sigma$')\n",
    "    plt.show(block=False)\n",
    "    plt.pause(2)\n",
    "    plt.close()\n",
    "    \n",
    "    return data_dn\n",
    "\n",
    "def get_Xy(data, D=12):\n",
    "    # make X and y\n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    N = len(data)\n",
    "    assert N>D, 'N should be larger than D, where N is len(data)'\n",
    "    for ii in range(N-D-1):\n",
    "        X_l.append(data[ii:ii+D])\n",
    "        y_l.append(data[ii+D])\n",
    "    X = np.array(X_l)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    y = np.array(y_l)\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6장 AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "\n",
    "class AE(models.Model):\n",
    "    def __init__(self, x_nodes, z_dim):\n",
    "        x_shape = (x_nodes, )\n",
    "        x = layers.Input(shape=x_shape) # 입력 계층\n",
    "        z = layers.Dense(z_dim, activation='relu')(x)   # 은닉 계층\n",
    "        y = layers.Dense(x_nodes, activation='sigmoid')(z)  # 출력 계층\n",
    "        super().__init__(x, y)\n",
    "        self.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "        self.x = x\n",
    "        self.z = z\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "    def Encoder(self):\t# output을 z로 불러와서 encoder 부분만으로 만든 모델\n",
    "        return models.Model(self.x, self.z)\n",
    "    \n",
    "    def Decoder(self):\t# deoder부분인 y_layer를 가져온 후 input을 맞춘 새로운 z를 입력한 모델\n",
    "        z_shape = (self.z_dim, )\n",
    "        z = layers.Input(shape=z_shape)\n",
    "        y_layer = self.layers[-1]\n",
    "        y = y_layer(z)\n",
    "\n",
    "        return models.Model(z, y)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_ae(autoencoder):\n",
    "    (_, _), (x_test, _) = mnist.load_data()\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    x_test = x_test.reshape((len(x_test), -1))\n",
    "    \n",
    "    encoder = autoencoder.Encoder()\n",
    "    decoder = autoencoder.Decoder()\n",
    "    encoded_imgs = encoder.predict(x_test)\n",
    "    decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "    n = 10\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(3, n, i+1)\n",
    "        plt.imshow(x_test[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "        ax = plt.subplot(3, n, i+1+n)\n",
    "        plt.stem(encoded_imgs[i].reshape(-1))\n",
    "        \n",
    "        ax = plt.subplot(3, n, i+1+n+n)\n",
    "        plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "\n",
    "    plt.show(block=False)\n",
    "    plt.pause(2)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from original.keraspp.skeras import plot_loss, plot_acc\n",
    "\n",
    "def main():\n",
    "    (x_train, _), (x_test, _) = mnist.load_data()\n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    x_train = x_train.reshape((len(x_train), -1))\n",
    "    x_test = x_test.reshape((len(x_test), -1))\n",
    "\n",
    "    x_nodes = 784\n",
    "    z_dim = 36\n",
    "\n",
    "    autoencoder = AE(x_nodes, z_dim)\n",
    "    history = autoencoder.fit(x_train, x_train, epochs=10, batch_size=1024, shuffle=True, validation_data=(x_test, x_test))\n",
    "    plot_acc(history)\n",
    "    plt.show(block=False)\n",
    "    plt.pause(2)\n",
    "    plt.close()\n",
    "\n",
    "    plot_loss(history)\n",
    "    plt.show(block=False)\n",
    "    plt.pause(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 합성곱 AE 모델링\n",
    "from keras import layers, models\n",
    "\n",
    "def Conv2D(filters, kernel_size, padding='same', activation='relu'):\n",
    "    return layers.Conv2D(filters, kernel_size, padding=padding, activation=activation)\n",
    "\n",
    "class AE(models.Model):\n",
    "    def __init__(self, org_shape):\n",
    "        # Input\n",
    "        original = layers.Input(shape=org_shape)\n",
    "        x = Conv2D(4, (3,3))(original)\n",
    "        x = layers.MaxPooling2D((2,2), padding='same')(x)\n",
    "        x = Conv2D(8, (3,3))(x)\n",
    "        x = layers.MaxPooling2D((2,2), padding='same')(x)\n",
    "\n",
    "        z = Conv2D(1, (7,7))(x)  # encoding의 output이자 decoding의 input\n",
    "\n",
    "        y = Conv2D(16, (3,3))(z)\n",
    "        y = layers.UpSampling2D((2,2))(y)\n",
    "        y = Conv2D(8, (3,3))(y)\n",
    "        y = layers.UpSampling2D((2,2))(y)\n",
    "        y = Conv2D(4, (3,3))(y)\n",
    "\n",
    "        decoded = Conv2D(1, (3,3), activation='sigmoid')(y)\n",
    "\n",
    "        super().__init__(original, decoded)\n",
    "        self.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "\n",
    "from practice_03 import DATA\n",
    "from original.keraspp.skeras import plot_loss, plot_acc\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend\n",
    "\n",
    "def show_ae(autoencoder, data):\n",
    "    x_test = data.X_test\n",
    "    decoded_imgs = autoencoder.predict(x_test)\n",
    "    print(decoded_imgs.shape, data.X_test.shape)\n",
    "\n",
    "    if backend.image_data_format() == 'channels_first':\n",
    "        N, n_ch, n_i, n_j = x_test.shape\n",
    "    else:\n",
    "        N, n_i, n_j, n_ch = x_test.shape\n",
    "    \n",
    "    x_test = x_test.reshape(N, n_i, n_j)\n",
    "    decoded_imgs = decoded_imgs.reshape(decoded_imgs.shape[0], n_i, n_j)\n",
    "\n",
    "    n = 10\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, n, i+1)\n",
    "        plt.imshow(x_test[i])\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        ax = plt.subplot(2, n, i+1+n)\n",
    "        plt.imshow(decoded_imgs[i])\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "def main(epochs=20, batch_size=128):\n",
    "    data = DATA()\n",
    "    autoencoder = AE(data.input_shape)\n",
    "    history = autoencoder.fit(data.X_train, data.X_train, epochs=epochs, batch_size=batch_size, shuffle=True, validation_split=0.2)\n",
    "    # history = autoencoder.fit(data.X_train, data.X_train, epochs=epochs, batch_size=batch_size, shuffle=True, validation_data=(data.X_test, data.X_test))\n",
    "\n",
    "    plot_acc(history)\n",
    "    plt.show(block=False)\n",
    "    plt.pause(2)\n",
    "    plt.close()\n",
    "\n",
    "    plot_loss(history)\n",
    "    plt.show(block=False)\n",
    "    plt.pause(2)\n",
    "    plt.close()\n",
    "\n",
    "    show_ae(autoencoder, data)\n",
    "    plt.show(block=False)\n",
    "    plt.pause(2)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7장 GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models\n",
    "from keras.layers import Dense, Conv1D, Reshape, Flatten, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "def main():\n",
    "    machine = Machine(n_batch=1, ni_D=100)\n",
    "    machine.run(n_repeat=200, n_show=200, n_test=100)\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, mu, sigma, ni_D):\n",
    "        self.real_sample = lambda n_batch: np.random.normal(mu, sigma, (n_batch, ni_D)) # 허구 데이터\n",
    "        self.in_sample = lambda n_batch: np.random.rand(n_batch, ni_D)   # 노이즈\n",
    "\n",
    "class Machine:\n",
    "    def __init__(self, n_batch=10, ni_D=100):\n",
    "        data_mean = 4\n",
    "        data_stddev = 1.25\n",
    "        self.data = Data(data_mean, data_stddev, ni_D)\n",
    "        \n",
    "        self.gan = GAN(ni_D=ni_D, nh_D=50, nh_G=50)\n",
    "        self.n_batch = n_batch\n",
    "        self.n_iter_D = 1\n",
    "        self.n_iter_G = 5\n",
    "\n",
    "    def run_epochs(self, epochs, n_test):\n",
    "                   self.train((epochs))\n",
    "                   self.test_and_show(n_test)\n",
    "\n",
    "    def run(self, n_repeat=30000//200, n_show=200, n_test=100):\n",
    "        for ii in range(n_repeat):\n",
    "            print('Stage', ii, '(Epoch: {}'.format(ii*n_show))\n",
    "            self.run_epochs(n_show, n_test)\n",
    "            plt.show(block=False)\n",
    "            plt.pause(2)\n",
    "            plt.close()\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.train_each()\n",
    "\n",
    "    def train_each(self):\n",
    "        for it in range(self.n_iter_D):\n",
    "            self.train_D()\n",
    "        for it in range(self.n_iter_G):\n",
    "            self.train_GD()\n",
    "    \n",
    "    def test(self, n_test):\n",
    "        gan = self.gan\n",
    "        data = self.data\n",
    "        Z = data.in_sample(n_test)\n",
    "        Gen = gan.G.predict(Z)\n",
    "\n",
    "        return Gen, Z\n",
    "    \n",
    "    def train_D(self):\n",
    "        gan = self.gan\n",
    "        n_batch = self.n_batch\n",
    "        data = self.data\n",
    "\n",
    "        Real = data.real_sample(n_batch)    # (n_batch, ni_D)\n",
    "        Z = data.in_sample(n_batch)         # (n_batch, ni_D)\n",
    "        Gen = gan.G.predict(Z)              # (n_batch, ni_D)\n",
    "        gan.D.trainable = True\n",
    "        gan.D_train_on_batch(Real, Gen)\n",
    "    \n",
    "    def train_GD(self):\n",
    "        gan = self.gan\n",
    "        n_batch = self.n_batch\n",
    "        data = self.data\n",
    "        Z = data.in_sample(n_batch)\n",
    "\n",
    "        gan.D.trainable = False\n",
    "        gan.GD_train_on_batch(Z)\n",
    "    \n",
    "    def test_and_show(self, n_test):\n",
    "        data = self.data\n",
    "        Gen, Z = self.test(n_test)\n",
    "        Real = data.real_sample(n_test)\n",
    "        self.show_hist(Real, Gen, Z)\n",
    "        Machine.print_stat(Real, Gen)\n",
    "\n",
    "    def show_hist(self, Real, Gen, Z):\n",
    "        plt.hist(Real.reshape(-1), histtype='step', label='Real')\n",
    "        plt.hist(Gen.reshape(-1), histtype='step', label='Generated')\n",
    "        plt.hist(Z.reshape(-1), histtype='step', label='Input')\n",
    "        plt.legend(loc=0)\n",
    "    \n",
    "    @staticmethod   # 인스턴스에 접근하지 않고 바로 함수로써 사용할 수 있게 하는 데코레이터\n",
    "    def print_stat(Real, Gen):\n",
    "        def stat(d):\n",
    "            return (np.mean(d), np.std(d))\n",
    "        print('Mean and Std of Real:', stat(Real))\n",
    "        print('Mean and Std of Gen:', stat(Gen))\n",
    "\n",
    "def add_decorate(x):\n",
    "    m = K.mean(x, axis=-1, keepdims=True)\n",
    "    d = K.square(x - m)\n",
    "\n",
    "    return K.concatenate([x, d], axis=-1)\n",
    "\n",
    "def add_decorate_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape)==2\n",
    "    shape[1] *= 2\n",
    "    return tuple(shape)\n",
    "\n",
    "lr = 2e-4\n",
    "adam = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "def model_compile(model):\n",
    "    return model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "\n",
    "class GAN:\n",
    "    def __init__(self, ni_D, nh_D, nh_G):\n",
    "        self.ni_D = ni_D\n",
    "        self.nh_D = nh_D\n",
    "        self.nh_G = nh_G\n",
    "\n",
    "        self.D = self.gen_D()\n",
    "        self.G = self.gen_G()\n",
    "        self.GD = self.make_GD()\n",
    "\n",
    "    def gen_D(self):\n",
    "        ni_D = self.ni_D\n",
    "        nh_D = self.nh_D\n",
    "        \n",
    "        D = models.Sequential()\n",
    "        D.add(Lambda(add_decorate, output_shape=add_decorate_shape, input_shape=(ni_D, )))\n",
    "        D.add(Dense(nh_D, activation='relu'))\n",
    "        D.add(Dense(nh_D, activation='relu'))\n",
    "        D.add(Dense(1, activation='sigmoid'))\n",
    "        model_compile(D)\n",
    "         \n",
    "        return D\n",
    "\n",
    "    \n",
    "    def gen_G(self):\n",
    "        ni_D = self.ni_D\n",
    "        nh_G = self.nh_G\n",
    "        G = models.Sequential()\n",
    "        G.add(Reshape((ni_D, 1), input_shape=(ni_D, )))\n",
    "        G.add(Conv1D(nh_G, 1, activation='relu'))\n",
    "        G.add(Conv1D(nh_G, 1, activation='sigmoid'))\n",
    "        G.add(Conv1D(1,1))\n",
    "        G.add(Flatten())\n",
    "        model_compile(G)\n",
    "        \n",
    "        return G\n",
    "\n",
    "    def make_GD(self):\n",
    "        G, D = self.G, self.D\n",
    "        GD = models.Sequential()\n",
    "        GD.add(G)\n",
    "        GD.add(D)\n",
    "        D.trainable = False\n",
    "        model_compile(GD)\n",
    "        D.trainable = True\n",
    "\n",
    "        return GD\n",
    "    \n",
    "    def D_train_on_batch(self, Real, Gen):\n",
    "        D = self.D\n",
    "        X = np.concatenate([Real, Gen], axis=0)\n",
    "        y = np.array([1]*Real.shape[0] + [0]*Gen.shape[0])\n",
    "        D.train_on_batch(X, y)\n",
    "    \n",
    "    def GD_train_on_batch(self, Z):\n",
    "        GD = self.GD\n",
    "        y = np.array([1]*Z.shape[0])\n",
    "        GD.train_on_batch(Z, y)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 합성곱 계층 GAN\n",
    "from keras.datasets import mnist\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math, os\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "print(K.image_data_format)\n",
    "\n",
    "def mse_4d(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=(1,2,3))\n",
    "\n",
    "def mse_4d_tf(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true), axis=(1,2,3))\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()  # 인자값을 처리하는 클래스인데 용처를 잘 모르겠음\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=16, help='Batch size for the networks')\n",
    "    parser.add_argument('--epochs', type=int, default=1000, help='Epochs for the networks')\n",
    "    parser.add_argument('--output_fold', type=str, default='GAN_OUT', help='Output fold to save the results')\n",
    "    parser.add_argument('--input_dim', type=int, default=10, help='Input dimension for the generator')\n",
    "    parser.add_argument('--n_train', type=int, default=32, help='The number of training data')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "\n",
    "from keras import models, layers, optimizers\n",
    "\n",
    "class GAN(models.Sequential):\n",
    "    def __init__(self, input_dim=64):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.generator = self.GENERATOR()\n",
    "        self.discriminator = self.DISCRIMINATOR()\n",
    "\n",
    "        self.add(self.generator)\n",
    "        self.discriminator.trainable = False\n",
    "        self.add(self.discriminator)\n",
    "        self.compile_all()\n",
    "    \n",
    "    def compile_all(self):\n",
    "        d_optim = optimizers.SGD(learning_rate=0.0005, momentum=0.9, nesterov=True)\n",
    "        g_optim = optimizers.SGD(learning_rate=0.0005, momentum=0.9, nesterov=True)\n",
    "\n",
    "        self.generator.compile(loss=mse_4d_tf, optimizer='SGD')\n",
    "        self.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "        self.discriminator.trainable = True\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "    \n",
    "    def GENERATOR(self):\n",
    "        input_dim = self.input_dim\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(1024, activation='tanh', input_dim=input_dim))\n",
    "        model.add(layers.Dense(128*7*7, activation='tanh'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Reshape((128,7,7), input_shape=(128*7*7, )))\n",
    "        model.add(layers.UpSampling2D(size=(2,2)))\n",
    "        model.add(layers.Conv2D(64, (5,5), padding='same', activation='tanh'))\n",
    "        model.add(layers.UpSampling2D(size=(2,2)))\n",
    "        model.add(layers.Conv2D(1, (5,5), padding='same', activation='tanh'))\n",
    "\n",
    "        return model\n",
    "        \n",
    "    def DISCRIMINATOR(self):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(64, (5,5), padding='same', activation='tanh', input_shape=(1, 28, 28)))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(layers.Conv2D(128, (5,5), activation='tanh'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(1024, activation='tanh'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def get_z(self, ln):\n",
    "        input_dim = self.input_dim\n",
    "        return np.random.uniform(-1, 1, (ln, input_dim))\n",
    "    \n",
    "\n",
    "    def train_both(self, x):\n",
    "        ln = x.shape[0]\n",
    "        # First trial for training discriminator\n",
    "        z = self.get_z(ln)\n",
    "        w = self.generator.predict(z, verbose=0)\n",
    "        xw = np.concatenate((x, w))\n",
    "        y2 = [1] * ln + [0] * ln\n",
    "\n",
    "        # train_on_batch에서 발생하는 오류 해결 못함\n",
    "        # AttributeError: 'int' object has no attribute 'shape'\n",
    "\n",
    "        # d_loss = self.discriminator.train_on_batch(xw, y2)\n",
    "        d_loss = 0\n",
    "        z = self.get_z(ln)\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # g_loss = self.generator.train_on_batch(z, [1] * ln)\n",
    "        g_loss = 0\n",
    "        self.discriminator.trainable = True\n",
    "\n",
    "        return d_loss, g_loss\n",
    "\n",
    "\n",
    "\n",
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num) / width))\n",
    "    shape = generated_images.shape[2:]\n",
    "    image = np.zeros((height * shape[0], width * shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index / width)\n",
    "        j = index % width\n",
    "        image[i * shape[0]:(i + 1) * shape[0],\n",
    "        j * shape[1]:(j + 1) * shape[1]] = img[0, :, :]\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_x(X_train, index, BATCH_SIZE):\n",
    "    return X_train[index * BATCH_SIZE:(index + 1) * BATCH_SIZE]\n",
    "\n",
    "\n",
    "def save_images(generated_images, output_fold, epoch, index):\n",
    "    image = combine_images(generated_images)\n",
    "    image = image * 127.5 + 127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        output_fold + '/' +\n",
    "        str(epoch) + \"_\" + str(index) + \".png\")\n",
    "\n",
    "\n",
    "def load_data(n_train):\n",
    "    (X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n",
    "    return X_train[:n_train]\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    output_fold = args.output_fold\n",
    "    input_dim = args.input_dim\n",
    "    n_train = args.n_train\n",
    "\n",
    "    os.makedirs(output_fold, exist_ok=True)\n",
    "    print('Output_fold is', output_fold)\n",
    "\n",
    "    X_train = load_data(n_train)\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:])\n",
    "\n",
    "    gan = GAN(input_dim)\n",
    "\n",
    "    d_loss_ll = []\n",
    "    g_loss_ll = []\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of batches\", int(X_train.shape[0] / BATCH_SIZE))\n",
    "\n",
    "        d_loss_l = []\n",
    "        g_loss_l = []\n",
    "    \n",
    "        for index in range(int(X_train.shape[0] / BATCH_SIZE)):\n",
    "            x = get_x(X_train, index, BATCH_SIZE)\n",
    "            \n",
    "            d_loss, g_loss = gan.train_both(x)\n",
    "            d_loss_l.append(d_loss)\n",
    "            g_loss_l.append(g_loss)\n",
    "\n",
    "        if epoch%10 == 0 or epoch == epochs-1:\n",
    "            z = gan.get_z(x.shape[0])\n",
    "            w = gan.generator.predict(z, verbose=0)\n",
    "            save_images(w, output_fold, epoch, index)\n",
    "        \n",
    "        d_loss_ll.append(d_loss_l)\n",
    "        g_loss_ll.append(g_loss_l)\n",
    "    \n",
    "    gan.generator.save_weights(output_fold + '/' + 'generator', True)\n",
    "    gan.discriminator.save_weights(output_fold + '/' + 'discriminator', True)\n",
    "\n",
    "    np.savetxt(output_fold + '/' + 'd_loss', d_loss_ll)\n",
    "    np.savetxt(output_fold + '/' + 'g_loss', g_loss_ll)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n",
    "    train(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
