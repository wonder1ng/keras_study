{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1장 케라스 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "print(keras.backend.backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([0, 1, 2, 3, 4])\n",
    "y = x * 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(1, input_shape=(1, )))\n",
    "model.compile(optimizer='SGD', loss='mse')\n",
    "\n",
    "model.fit(x[:2], y[:2], epochs=1000, verbose=0)\n",
    "\n",
    "print('Targets:', y[2:])\n",
    "print('Predictions:', model.predict(x[2:]).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2장 ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분산 방식 모델링, 함수형 구현\n",
    "from keras import layers, models\n",
    "\n",
    "x = layers.Input(shape=(None, ))\n",
    "h = layers.Activation('relu')(layers.Dense(None)(x))\n",
    "y = layers.Activation('softmax')(layers.Dense(None)(h))\n",
    "\n",
    "model = models.Model(x, y)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연쇄 방식 모델링, 함수형 구현\n",
    "model = models.Sequential() # 모델 구조 정의 전 Sequential로 초기화\n",
    "model.add(layers.Dense(None, activation='relu', input_shape=(None, )))\n",
    "model.add(layers.Dense(None, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분산 방식 모델링, 객체지향형 구현\n",
    "class ANN(models.Model):\n",
    "    def __init__(self, input_num, hidden_num, output_num, **kwargs):\n",
    "        hidden = layers.Dense(hidden_num)\n",
    "        output = layers.Dense(output_num)\n",
    "        relu = layers.Activation('relu')\n",
    "        softmax = layers.Activation('softmax')\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "model = ANN(input_num, hidden_num, output_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연쇄 방식 모델링, 객체지향형 구현\n",
    "class ANN(models.Sequential):\n",
    "    def __init__(self, input_num, hidden_num, output_num, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.add(layers.Dense(hidden_num, activation='relu', input_shape=(input_num, )))\n",
    "        self.add(layers.Dense(output_num, activation='softmax'))\n",
    "        self.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from keras import datasets  #mnist\n",
    "from keras.datasets import mnist    # 이렇게 불러와야 실행됨\n",
    "from keras.utils import np_utils # to_categorical\n",
    "\n",
    "def Data_func():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "    L, W, H = X_train.shape\n",
    "    X_train = X_train.reshape(-1, W*H)\n",
    "    X_test = X_test.reshape(-1, W*H)\n",
    "\n",
    "    X_train = X_train/255.0\n",
    "    X_test = X_test/255.0\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc=0)\n",
    "\n",
    "def plot_acc(history):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_num = 784\n",
    "hidden_num = 100\n",
    "number_of_class = 10\n",
    "output_num = number_of_class\n",
    "model = ANN(input_num, hidden_num, output_num)\n",
    "(X_train, y_train), (X_test, y_test) = Data_func()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=100, validation_split=0.2)\n",
    "\n",
    "performance_test = model.evaluate(X_test, y_test, batch_size=100)\n",
    "print('Test loss and Accuracy -> {: .2f}, {: .2f}'.format(*performance_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)\n",
    "plt.show()\n",
    "plot_acc(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시계열\n",
    "from keras import layers, models\n",
    "\n",
    "class ANN(models.Model):\n",
    "    def __init__(self, Nin, Nh, Nout):\n",
    "        hidden = layers.Dense(Nh)\n",
    "        output = layers.Dense(Nout)\n",
    "        relu = layers.Activation('relu')\n",
    "\n",
    "        x = layers.Input(shape=(Nin, ))\n",
    "        h = relu(hidden(x))\n",
    "        y = output(h)\n",
    "\n",
    "        super.__init__(x, y)\n",
    "        self.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "# 보스턴 집값 데이터\n",
    "from keras.datasets import boston_housing\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def Data_func():\n",
    "    (X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "# 시각화\n",
    "from original.keraspp.skeras import plot_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main():\n",
    "    Nin = 13\n",
    "    Nh = 5\n",
    "    Nout = 1\n",
    "\n",
    "    model = ANN(Nin, Nh, Nout)\n",
    "    (X_train, y_train), (X_test, y_test) = Data_func()\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=100, validation_split=0.2, verbose=2)\n",
    "\n",
    "    performance_test = model.evaluate(X_test, y_test, batch_size=100)\n",
    "    print('\\nTest Loss -> {: .2f}'.format(performance_test))\n",
    "\n",
    "    plot_loss()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3장 DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "\n",
    "Nin = 784\n",
    "Nh_l = [100, 50]\n",
    "number_of_class = 10\n",
    "Nout = number_of_class\n",
    "\n",
    "class DNN(models.Sequential):\n",
    "    def __init__(self, Nin, Nh_1, Nout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.add(layers.Dense(Nh_1[0], activation='relu', input_shape=(Nin, ), name='Hidden-1'))\n",
    "        self.add(layers.Dropout(0.2))\n",
    "        self.add(layers.Dense(Nh_1[1], activation='relu', name='Hidden-2'))\n",
    "        self.add(layers.Dropout(0.2))\n",
    "        self.add(layers.Dense(Nout, activation='softmax'))\n",
    "\n",
    "        self.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils # to_categorical\n",
    "\n",
    "def Data_func():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "    L, W, H = X_train.shape\n",
    "    X_train = X_train.reshape(-1, W*H)\n",
    "    X_test = X_test.reshape(-1, W*H)\n",
    "\n",
    "    X_train = X_train/255.0\n",
    "    X_test = X_test/255.0\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = Data_func()\n",
    "\n",
    "model = DNN(Nin, Nh_l, Nout)\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=100, validation_split=0.2)\n",
    "performance_test = model.evaluate(X_test, y_test, batch_size=100)\n",
    "print('Test Loss and Accuracy ->', performance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬러 이미지\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def Data_func():\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "    L, W, H, C = X_train.shape\n",
    "    X_train = X_train.reshape(-1, W*H*C)\n",
    "    X_test = X_test.reshape(-1, W*H*C)\n",
    "\n",
    "    X_train = X_train/255.0\n",
    "    X_test = X_test/255.0\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "\n",
    "# 모델링\n",
    "from keras import layers, models\n",
    "\n",
    "class DNN(models.Sequential):\n",
    "    def __init__(self, Nin, Nh_l, Pd_l, Nout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.add(layers.Dense(Nh_l[0], activation='relu', input_shape=(Nin, ), name='Hiiden-1'))\n",
    "        self.add(layers.Dropout(Pd_l[0]))\n",
    "        self.add(layers.Dense(Nh_l[1], activation='relu', input_shape=(Nin, ), name='Hiiden-2'))\n",
    "        self.add(layers.Dropout(Pd_l[1]))\n",
    "        self.add(layers.Dense(Nout, activation='softmax'))\n",
    "        self.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# 학습 및 시각화\n",
    "from original.keraspp.skeras import plot_loss, plot_acc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Nh_l = [100, 50]\n",
    "Pd_l = [0.0, 0.0]\n",
    "number_of_class = 10\n",
    "Nout = number_of_class\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = Data_func()\n",
    "model = DNN(X_train.shape[1], Nh_l, Pd_l, Nout)\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=100, validation_split=0.2)\n",
    "performance_test = model.evaluate(X_test, y_test, batch_size=100)\n",
    "print('Test Loss and Accuracy ->', performance_test)\n",
    "\n",
    "plot_acc(history)\n",
    "plt.show(block=False)\n",
    "plt.pause(2)\n",
    "plt.close()\n",
    "plot_loss(history)\n",
    "plt.show(block=False)\n",
    "plt.pause(2)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4장 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models, layers\n",
    "import keras.backend\n",
    "import keras.optimizers\n",
    "\n",
    "class CNN(models.Sequential):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolutionL 특징점 찾기\n",
    "        self.add(layers.Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=input_shape))\n",
    "        self.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "        self.add(layers.MaxPooling2D(pool_size=(2,2)))  # pool_size: 1개의 값으로 변화할 크기\n",
    "        self.add(layers.Dropout(0.25))\n",
    "        self.add(layers.Flatten())\n",
    "\n",
    "        # dense: 실질적 분류 작업\n",
    "        self.add(layers.Dense(128, activation='relu'))\n",
    "        self.add(layers.Dropout(0.5))\n",
    "        self.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        self.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['acc'])\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 채널의 위치는 image_data_format에 따름\n",
    "print(keras.backend.backend_config.image_data_format())\n",
    "\n",
    "# 1 = 채널의 크기\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "# X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "X_test = X_test.reshape(*X_test.shape, 1)\n",
    "input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
    "\n",
    "## 채널의 위치가 앞 단에 존재할 경우\n",
    "# X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
    "# X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2])\n",
    "# input_shape = (1, X_train.shape[1], X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models, layers\n",
    "import keras.backend\n",
    "import keras.optimizers\n",
    "\n",
    "class CNN(models.Sequential):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolutionL 특징점 찾기\n",
    "        self.add(layers.Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=input_shape))\n",
    "        self.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "        self.add(layers.MaxPooling2D(pool_size=(2,2)))  # pool_size: 1개의 값으로 변화할 크기\n",
    "        self.add(layers.Dropout(0.25))\n",
    "        self.add(layers.Flatten())\n",
    "\n",
    "        # dense: 실질적 분류 작업\n",
    "        self.add(layers.Dense(128, activation='relu'))\n",
    "        self.add(layers.Dropout(0.5))\n",
    "        self.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        self.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['acc'])\n",
    "\n",
    "from keras.datasets import mnist\n",
    "class DATA():\n",
    "    def __init__(self):\n",
    "        num_classes = 10\n",
    "\n",
    "        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "        \n",
    "        # 채널의 위치는 image_data_format에 따름\n",
    "        print(keras.backend.backend_config.image_data_format())\n",
    "\n",
    "        # 1 = 채널의 크기\n",
    "        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "        # X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "        X_test = X_test.reshape(*X_test.shape, 1)\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
    "\n",
    "        ## 채널의 위치가 앞 단에 존재할 경우\n",
    "        # X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
    "        # X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2])\n",
    "        # input_shape = (1, X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "        X_train = X_train.astype('float32')\n",
    "        X_test = X_test.astype('float32')\n",
    "        X_train /= 255\n",
    "        X_test /= 255\n",
    "\n",
    "        y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "        y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_test, self.y_test = X_test, y_test\n",
    "\n",
    "data = DATA()\n",
    "model = CNN(data.input_shape, data.num_classes)\n",
    "history = model.fit(data.X_train, data.y_train, batch_size=128, epochs=10, validation_split=0.2)\n",
    "score = model.evaluate(data.X_test, data.y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "plot_acc(history)\n",
    "plt.show(block=False)\n",
    "plt.pause(2)\n",
    "plt.close()\n",
    "\n",
    "plot_loss(history)\n",
    "plt.show(block=False)\n",
    "plt.pause(2)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬러\n",
    "import keras.backend\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from original.keraspp import skeras, sfile\n",
    "\n",
    "class CNN(Model):\n",
    "    def __init__(model, nb_classes, in_shape=None):\n",
    "        model.nb_classes = nb_classes\n",
    "        model.in_shape = in_shape\n",
    "        model.build_model()\n",
    "        super().__init__(model.x, model.y)\n",
    "        model.cl_part = Model(model.x, model.z_cl)\n",
    "        model.fl_part = Model(model.x, model.z_fl)\n",
    "        model.compile()\n",
    "    \n",
    "    def build_model(model):\n",
    "        nb_classes = model.nb_classes\n",
    "        in_shape = model.in_shape\n",
    "\n",
    "        x = Input(in_shape)\n",
    "        h = Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=in_shape)(x)\n",
    "        h = Conv2D(64, (3,3), activation='relu')(h)\n",
    "        h = MaxPooling2D(pool_size=(2,2))(h)\n",
    "        h = Dropout(0.25)(h)\n",
    "        h = Flatten()(h)\n",
    "        z_cl = h\n",
    "\n",
    "        h = Dense(128, activation='relu')(h)\n",
    "        h = Dropout(0.5)(h)\n",
    "        z_fl = h\n",
    "\n",
    "        y = Dense(nb_classes, activation='softmax', name='preds')(h)\n",
    "        model.z_cl = z_cl\n",
    "        model.z_fl = z_fl\n",
    "        model.x, model.y = x, y\n",
    "\n",
    "    def compile(model):\n",
    "        Model.compile(model, loss='categorical_crossentropy', optimizer='adadelta', metrics=['acc'])\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self, X, y, nb_classes, scaling=True, test_size=0.2, random_state=0):\n",
    "        self.X = X\n",
    "        self.add_channels()\n",
    "        X = self.X\n",
    "\n",
    "        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "        X_train = X_train.astype('float32')\n",
    "        X_test = X_test.astype('float32')\n",
    "\n",
    "        if scaling:\n",
    "            # scaling to have (0, 1) for each feature (each pixel)\n",
    "            scaler = MinMaxScaler()\n",
    "            n = X_train.shape[0]\n",
    "            X_train = scaler.fit_transform(X_train.reshape(n, -1)).reshape(X_train.shape)\n",
    "            n = X_test.shape[0]\n",
    "            X_test = scaler.transform(X_test.reshape(n, -1)).reshape(X_test.shape)\n",
    "            self.scaler = scaler\n",
    "        \n",
    "        y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "        y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "        \n",
    "        self.X_train, self.X_test = X_train, X_test\n",
    "        self.y_train, self.y_test = y_train, y_test\n",
    "    \n",
    "    def add_channels(self):\n",
    "        X = self.X\n",
    "\n",
    "        if len(X.shape) == 3:\n",
    "            N, img_rows, img_cols = X.shape\n",
    "\n",
    "            if K.image_data_format() == 'channels_first':\n",
    "                X = X.reshape(X.shape[0], 1, img_rows, img_cols)\n",
    "                input_shape = (1, img_rows, img_cols)\n",
    "            else:\n",
    "                X = X.reshape(X.shape[0], img_rows, img_cols, 1)\n",
    "                input_shape = (img_rows, img_cols, 1)\n",
    "        else:\n",
    "            input_shape = X.shape[1:]   # channel is already included\n",
    "        \n",
    "        self.X = X\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "class Machine():\n",
    "    def __init__(self, X, y, nb_classes=2, fig=True):\n",
    "        self.nb_classes = nb_classes\n",
    "        self.set_data(X, y)\n",
    "        self.set_model()\n",
    "        self.fig = fig\n",
    "    \n",
    "    def set_data(self, X, y):\n",
    "        nb_classes = self.nb_classes\n",
    "        self.data = DataSet(X, y, nb_classes)\n",
    "    \n",
    "    def set_model(self):\n",
    "        nb_classes = self.nb_classes\n",
    "        data = self.data\n",
    "        self.model = CNN(nb_classes=nb_classes, in_shape=data.input_shape)\n",
    "    \n",
    "    def fit(self, nb_epoch=10, batch_size=128, verbose=1):\n",
    "        data = self.data\n",
    "        model = self.model\n",
    "        history = model.fit(data.X_train, data.y_train, batch_size=batch_size, epochs=nb_epoch, verbose=verbose, validation_data=(data.X_test, data.y_test))\n",
    "        return history\n",
    "\n",
    "    def run(self, nb_epoch=10, batch_size=128, verbose=1):\n",
    "        data = self.data\n",
    "        model = self.model\n",
    "        fig = self.fig\n",
    "    \n",
    "        history = self.fit(nb_epoch=nb_epoch, batch_size=batch_size, verbose=verbose)\n",
    "        score = model.evaluate(data.X_test, data.y_test, verbose=0)\n",
    "\n",
    "        print('Confusion matrix')\n",
    "        y_test_pred = model.predict(data.X_test, verbose=0)\n",
    "        y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "        print(metrics.confusion_matrix(np.argmax(data.y_test, axis=1), y_test_pred))\n",
    "\n",
    "        print('Test score:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        suffix = sfile.unique_filename('datatime')\n",
    "        foldname = 'output_' + suffix\n",
    "        os.makedirs(foldname)\n",
    "        skeras.save_history_history('history_history.npy', history.history, fold=foldname)\n",
    "        model.save_weights(os.path.join(foldname, 'dl_model.h5'))\n",
    "        print('Output results are saved in', foldname)\n",
    "\n",
    "        if fig:\n",
    "            plt.figure(figsize=(12,4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            skeras.plot_acc(history)\n",
    "            plt.subplot(1, 2, 2)\n",
    "            skeras.plot_loss(history)\n",
    "            plot_acc(history)\n",
    "            plt.show(block=False)\n",
    "            plt.pause(2)\n",
    "            plt.close()\n",
    "\n",
    "        self.history = history\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "import keras\n",
    "assert keras.backend.image_data_format() == 'channels_last'\n",
    "from original.keraspp import aicnn\n",
    "\n",
    "class Machine(Machine):\n",
    "    def __init__(self):\n",
    "        (X, y), (X_test, y_test) = cifar10.load_data()\n",
    "        super().__init__(X=X, y=y, nb_classes=10)\n",
    "    \n",
    "def main():\n",
    "    m = Machine()\n",
    "    m.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5장 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Training stage\n",
      "====================\n",
      "Epoch 1/3\n",
      "49/49 [==============================] - 19s 341ms/step - loss: 0.6020 - acc: 0.6710 - val_loss: 0.4355 - val_acc: 0.8116\n",
      "Epoch 2/3\n",
      "49/49 [==============================] - 18s 370ms/step - loss: 0.3275 - acc: 0.8616 - val_loss: 0.3542 - val_acc: 0.8429\n",
      "Epoch 3/3\n",
      "49/49 [==============================] - 18s 375ms/step - loss: 0.2205 - acc: 0.9180 - val_loss: 0.3765 - val_acc: 0.8352\n",
      "49/49 [==============================] - 2s 37ms/step - loss: 0.3765 - acc: 0.8352\n",
      "Test performance: accuracy0.8352400064468384, loss=0.3765231966972351\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function   # 파이썬 2와 3 간의 호환성 위한 패키지\n",
    "# 파이썬 2에선 print를 괄호 없이 사용하지만 이 함수를 호출함으로써 함수로써(괄호와 함꼐) 사용하게 함\n",
    "\n",
    "# 자연어\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import pad_sequences\n",
    "from keras.datasets import imdb\n",
    "from keras import layers, models\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, max_features=20000, maxlen=80):\n",
    "        (x_train, self.y_train), (x_test, self.y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "        # x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "        # x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "        # pad_sequences의 경로가 바뀜\n",
    "        self.x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "        self.x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "class RNN_LSTM(models.Model):\n",
    "    def __init__(self, max_features, maxlen):\n",
    "        # model = models.Sequential()\n",
    "        # model.add(layers.Embedding(max_features, 128))\n",
    "        # model.add(layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "        # model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "        x = layers.Input((maxlen, ))\n",
    "        h = layers.Embedding(max_features, 128)(x)\n",
    "        h = layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2)(h)\n",
    "        y = layers.Dense(1, activation='sigmoid')(h)\n",
    "        super().__init__(x, y)\n",
    "        self.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "class Machine:\n",
    "    def __init__(self, max_features=20000, maxlen=80):\n",
    "        self.data = Data(max_features, maxlen)\n",
    "        self.model = RNN_LSTM(max_features, maxlen)\n",
    "    \n",
    "    def run(self, epochs=3, batch_size=32):\n",
    "        data = self.data\n",
    "        model = self.model\n",
    "\n",
    "        print('Training stage')\n",
    "        print('====================')\n",
    "        model.fit(data.x_train, data.y_train, batch_size=batch_size, epochs=epochs, validation_data=(data.x_test, data.y_test))\n",
    "        score, acc = model.evaluate(data.x_test, data.y_test, batch_size=batch_size)\n",
    "        print('Test performance: accuracy{0}, loss={1}'.format(acc, score))\n",
    "\n",
    "def main():\n",
    "    m = Machine()\n",
    "    m.run(batch_size=512)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시계열\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from keras import models, layers\n",
    "\n",
    "from original.keraspp import skeras\n",
    "\n",
    "def main():\n",
    "    machine = Machine()\n",
    "    machine.run(epochs=400)\n",
    "\n",
    "class Machine():\n",
    "    def __init__(self):\n",
    "        self.data = Dataset()\n",
    "        shape = self.data.X.shape[1:]\n",
    "        self.model = rnn_model(shape)\n",
    "    \n",
    "    def run(self, epochs=400):\n",
    "        d = self.data\n",
    "        X_train, X_test = d.X_train, d.X_test\n",
    "        y_train, y_test = d.y_train, d.y_test\n",
    "        X, y = d.X, d.y\n",
    "        \n",
    "        m = self.model\n",
    "        h = m.fit(X_train, y_train, epochs=epochs, validation_data=[X_test, y_test], verbose=0)\n",
    "        skeras.plot_loss(h)\n",
    "        plt.title('History of training')\n",
    "        plt.show(block=False)\n",
    "        plt.pause(2)\n",
    "        plt.close()\n",
    "\n",
    "        yp = m.predict(X_test)\n",
    "        print('Loss:', m.evaluate(X_test, y_test))\n",
    "        plt.plot(yp, label='Original')\n",
    "        plt.plot(y_test, label='Prediction')\n",
    "        plt.legend(loc=0)\n",
    "        plt.title('Validation Results')\n",
    "        plt.show(block=False)\n",
    "        plt.pause(2)\n",
    "        plt.close()\n",
    "\n",
    "        yp = m.predict(X_test).reshape(-1)\n",
    "        print('Loss:', m.evaluate(X_test, y_test))\n",
    "        print(yp.shape, y_test.shape)\n",
    "        df = pd.DataFrame()\n",
    "        df['Sample'] = list(range(len(y_test))) * 2\n",
    "        df['Normalized #Passengers'] = np.concatenate([y_test, yp], axis=0)\n",
    "        df['Type'] = ['Original'] * len(y_test) + ['Prediction'] * len(yp)\n",
    "\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        sns.barplot(x='Sample', y='Normalized #Passengers', hue='Type', data=df)\n",
    "        plt.ylabel('Normalized #Passengers')\n",
    "        plt.show(block=False)\n",
    "        plt.pause(2)\n",
    "        plt.close()\n",
    "\n",
    "        yp = m.predict(X)\n",
    "        plt.plot(yp, label='Original')\n",
    "        plt.plot(y, label='Prediction')\n",
    "        plt.legend(loc=0)\n",
    "        plt.title('All Results')\n",
    "        plt.show(block=False)\n",
    "        plt.pause(2)\n",
    "        plt.close()\n",
    "\n",
    "def rnn_model(shape):\n",
    "    m_x = layers.Input(shape=shape)\n",
    "    m_h = layers.LSTM(10)(m_x)\n",
    "    m_y = layers.Dense(1)(m_h)\n",
    "    m = models.Model(m_x, m_y)\n",
    "    m.compile('adam', 'mean_squared_error')\n",
    "    m.summary()\n",
    "\n",
    "    return m\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, fname='original/international-airline-passengers.csv', D=12):\n",
    "        data_dn = load_data(fname=fname)\n",
    "        X, y = get_Xy(data_dn, D=D)\n",
    "        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.X, self.y = X, y\n",
    "        self.X_train, self.X_test = X_train, X_test\n",
    "        self.y_train, self.y_test = y_train, y_test\n",
    "    \n",
    "def load_data(fname='original/international-airline-passengers.csv'):\n",
    "    dataset = pd.read_csv(fname, usecols=[1], engine='python', skipfooter=3)\n",
    "    data = dataset.values.reshape(-1)\n",
    "\n",
    "    plt.plot(data)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('#Passengers')\n",
    "    plt.title('Original Data')\n",
    "    plt.show(block=False)\n",
    "    plt.pause(2)\n",
    "    plt.close()\n",
    "\n",
    "    data_dn = (data - np.mean(data)) / np.std(data) / 5\n",
    "    plt.plot(data_dn)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Normalized #Passengers')\n",
    "    plt.title('Normalized data by $E[]$ and $5\\sigma$')\n",
    "    plt.show(block=False)\n",
    "    plt.pause(2)\n",
    "    plt.close()\n",
    "    \n",
    "    return data_dn\n",
    "\n",
    "def get_Xy(data, D=12):\n",
    "    # make X and y\n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    N = len(data)\n",
    "    assert N>D, 'N should be larger than D, where N is len(data)'\n",
    "    for ii in range(N-D-1):\n",
    "        X_l.append(data[ii:ii+D])\n",
    "        y_l.append(data[ii+D])\n",
    "    X = np.array(X_l)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    y = np.array(y_l)\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
